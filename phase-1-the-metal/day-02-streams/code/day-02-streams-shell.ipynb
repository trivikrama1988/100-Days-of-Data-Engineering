{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-day2",
   "metadata": {},
   "source": [
    "# **Day 2: Advanced Shell & Stream Processing ‚Äî Interactive Lab**\n",
    "\n",
    "**Objective:** Move from the Kernel (Day 1) to the Shell (Day 2). We will prove why the **Unix Philosophy** (small tools, pipes, streams) is the foundation of modern Data Engineering.\n",
    "\n",
    "### **Core Concepts:**\n",
    "1. **The Physics of Pipes:** RAM Buffers vs. Disk I/O.\n",
    "2. **Tool Mechanics:** Why `grep` is 10x faster than `awk`.\n",
    "3. **Parallelism:** Saturating CPUs with `xargs` and Multiprocessing.\n",
    "4. **Process Substitution:** Making streams look like files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-1-header",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-1-intro",
   "metadata": {},
   "source": [
    "## **üß™ Level 1: The Physics of Pipes (RAM vs Disk)**\n",
    "\n",
    "**Theory Recap:** \n",
    "* **Disk-Based:** `cmd1 > file; cmd2 < file`. This writes to the physical disk (Slow, High I/O).\n",
    "* **Stream-Based:** `cmd1 | cmd2`. This writes to a **64KB Kernel Ring Buffer** in RAM. (Fast, Zero Disk I/O).\n",
    "\n",
    "**Experiment:** We will simulate a \"Pipe Race\". We will process 500,000 records using intermediate files (Junior approach) vs. Pipes (Senior approach)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "code-1-data-gen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using existing input file.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# 1. Setup Data\n",
    "INPUT_FILE = \"large_input.txt\"\n",
    "TEMP_FILE = \"temp_intermediate.txt\"\n",
    "DATA_SIZE_LINES = 500_000\n",
    "\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(f\"Generating {DATA_SIZE_LINES} lines of data...\")\n",
    "    with open(INPUT_FILE, \"w\") as f:\n",
    "        for i in range(DATA_SIZE_LINES):\n",
    "            # Write: \"Row <ID> <RandomNum>\"\n",
    "            f.write(f\"Row {i} {i*2}\\n\")\n",
    "    print(\"‚úÖ Input file ready.\")\n",
    "else:\n",
    "    print(\"‚úÖ Using existing input file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "code-1-disk-bench",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [1] Disk-Based Approach (Intermediate File) ---\n",
      "Time: 1.1709s\n"
     ]
    }
   ],
   "source": [
    "print(\"--- [1] Disk-Based Approach (Intermediate File) ---\")\n",
    "start = time.time()\n",
    "\n",
    "# Step 1: Filter (grep equivalent) -> Write to Disk\n",
    "with open(INPUT_FILE, 'r') as fin, open(TEMP_FILE, 'w') as fout:\n",
    "    for line in fin:\n",
    "        if 'Row' in line:\n",
    "            fout.write(line)\n",
    "\n",
    "# Step 2: Process (awk equivalent) -> Read from Disk\n",
    "count = 0\n",
    "with open(TEMP_FILE, 'r') as fin:\n",
    "    for line in fin:\n",
    "        parts = line.split()\n",
    "        if len(parts) > 2:\n",
    "            count += 1\n",
    "\n",
    "disk_time = time.time() - start\n",
    "print(f\"Time: {disk_time:.4f}s\")\n",
    "\n",
    "# Cleanup temp file\n",
    "if os.path.exists(TEMP_FILE): os.remove(TEMP_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "code-1-pipe-bench",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [2] Stream-Based Approach (Pipe Logic) ---\n",
      "Time: 0.4879s\n",
      "\n",
      "üöÄ Speedup: 2.40x faster using Streams\n"
     ]
    }
   ],
   "source": [
    "print(\"--- [2] Stream-Based Approach (Pipe Logic) ---\")\n",
    "start = time.time()\n",
    "\n",
    "# Single Pass: Read once, process in memory, never write intermediate\n",
    "count = 0\n",
    "with open(INPUT_FILE, 'r') as fin:\n",
    "    for line in fin:\n",
    "        # Logic 1: Filter\n",
    "        if 'Row' in line:\n",
    "            # Logic 2: Process\n",
    "            parts = line.split()\n",
    "            if len(parts) > 2:\n",
    "                count += 1\n",
    "\n",
    "pipe_time = time.time() - start\n",
    "print(f\"Time: {pipe_time:.4f}s\")\n",
    "\n",
    "print(f\"\\nüöÄ Speedup: {disk_time / pipe_time:.2f}x faster using Streams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-1-examples",
   "metadata": {},
   "source": [
    "### **More Examples: The Power of Pipes**\n",
    "\n",
    "**Example 1: Streaming Compression (The Log Saver)**\n",
    "Imagine you have a 100GB log file. You want to compress it. \n",
    "* **Bad:** `cat log.txt > temp.txt; gzip temp.txt` (Writes 100GB to disk twice!)\n",
    "* **Good:** `cat log.txt | gzip > log.gz` (Compresses in memory buffers. Writes only compressed bytes to disk.)\n",
    "\n",
    "**Example 2: Network Streaming (Video)**\n",
    "You want to watch a movie being downloaded.\n",
    "* **Bad:** `wget movie.mp4`. Wait 10 mins. `vlc movie.mp4`.\n",
    "* **Good:** `wget -O - movie.mp4 | vlc -` (Watch while downloading. Data flows from Network -> RAM -> Player.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-1-reflection",
   "metadata": {},
   "source": [
    "### **üìù Analysis:**\n",
    "Why was the stream faster? \n",
    "1. **No Write Overhead:** We didn't spend time waiting for the hard drive head to write `temp_intermediate.txt`.\n",
    "2. **No Read Overhead:** We didn't have to read that temp file back into memory.\n",
    "3. **CPU Cache:** Data likely stayed in the CPU L1/L2 cache between the filter logic and the counting logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-2-header",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-2-intro",
   "metadata": {},
   "source": [
    "## **üîß Level 2: Tool Mechanics (grep vs awk)**\n",
    "\n",
    "**Theory Recap:** \n",
    "* **`grep`:** A \"Bouncer\". It checks simple patterns using highly optimized C code (Boyer-Moore algorithm). It treats text as raw bytes.\n",
    "* **`awk`:** An \"Accountant\". It parses *every single line* into fields (`$1`, `$2`...) before running your logic. This parsing is expensive.\n",
    "\n",
    "**Experiment:** We will search for a specific string (`9999`) in a 1,000,000 line file using both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "code-2-data-gen",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOL_FILE = \"tool_race.txt\"\n",
    "LINES = 1_000_000\n",
    "\n",
    "if not os.path.exists(TOOL_FILE):\n",
    "    print(\"Generating tool race data...\")\n",
    "    with open(TOOL_FILE, \"w\") as f:\n",
    "        for i in range(LINES):\n",
    "            f.write(f\"Log Entry {i} [INFO] System status normal with value {i}\\n\")\n",
    "    print(\"‚úÖ Data generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "code-2-grep-sim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simulation: grep (Scanning) ---\n",
      "Time: 0.6505s\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Simulation: grep (Scanning) ---\")\n",
    "start = time.time()\n",
    "count = 0\n",
    "with open(TOOL_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        # Grep Logic: Just check existence. No splitting.\n",
    "        if '9999' in line:\n",
    "            count += 1\n",
    "grep_time = time.time() - start\n",
    "print(f\"Time: {grep_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "code-2-awk-sim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Simulation: awk (Parsing) ---\n",
      "Time: 1.3430s\n",
      "\n",
      ">>> Result: grep logic was 2.1x faster.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Simulation: awk (Parsing) ---\")\n",
    "start = time.time()\n",
    "count = 0\n",
    "with open(TOOL_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        # Awk Logic: Pay the tax of splitting fields FIRST\n",
    "        fields = line.split()\n",
    "        # Then check the specific column\n",
    "        if len(fields) > 7 and '9999' in fields[7]: \n",
    "            count += 1\n",
    "awk_time = time.time() - start\n",
    "print(f\"Time: {awk_time:.4f}s\")\n",
    "\n",
    "print(f\"\\n>>> Result: grep logic was {awk_time / grep_time:.1f}x faster.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-2-examples",
   "metadata": {},
   "source": [
    "### **More Examples: Selecting the Right Tool**\n",
    "\n",
    "**Example 1: Finding an IP in Access Logs**\n",
    "* **`grep '192.168.1.1' access.log`**: Fastest. Just scans bytes. Good for \"Is this IP here?\"\n",
    "* **`awk '$1 == \"192.168.1.1\"' access.log`**: Slower. Splits every line by spaces first. Use only if you need to check specific columns (e.g., \"IP is in column 1 AND status is 404\").\n",
    "\n",
    "**Example 2: CSV Filtering**\n",
    "Imagine a CSV: `ID,Name,Age,Salary`.\n",
    "* **Task:** Find all rows containing \"Smith\".\n",
    "    * Use `grep \"Smith\"`. It ignores commas and just looks for the text.\n",
    "* **Task:** Find all rows where Salary > 50000.\n",
    "    * Use `awk -F, '$4 > 50000'`. `grep` cannot do math. You pay the performance cost for the math capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-3-header",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-3-intro",
   "metadata": {},
   "source": [
    "## **üöÄ Level 3: Parallelism (xargs & Multiprocessing)**\n",
    "\n",
    "**Theory Recap:** Standard tools (`grep`, `gzip`, Python scripts) are single-threaded. They use 1 CPU core. Modern servers have 64+ cores. \n",
    "\n",
    "**Goal:** Saturate the CPU by splitting work. In the Shell, we use `xargs -P`. In Python, `multiprocessing`.\n",
    "\n",
    "**Experiment:** We will run a CPU-heavy calculation (Sum of Squares) sequentially vs. parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-3-parallel-bench",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing 8 CPU-heavy tasks ---\n",
      "Sequential Time: 1.6900s\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def heavy_task(n):\n",
    "    # Simulate pure CPU work\n",
    "    count = 0\n",
    "    for i in range(2_000_000):\n",
    "        count += i * i\n",
    "    return count\n",
    "\n",
    "TASKS = list(range(8)) # 8 Heavy Tasks\n",
    "\n",
    "print(f\"--- Processing {len(TASKS)} CPU-heavy tasks ---\")\n",
    "\n",
    "# 1. Sequential\n",
    "start = time.time()\n",
    "for t in TASKS:\n",
    "    heavy_task(t)\n",
    "seq_time = time.time() - start\n",
    "print(f\"Sequential Time: {seq_time:.4f}s\")\n",
    "\n",
    "# 2. Parallel (4 Cores)\n",
    "start = time.time()\n",
    "with multiprocessing.Pool(processes=4) as pool:\n",
    "    pool.map(heavy_task, TASKS)\n",
    "par_time = time.time() - start\n",
    "print(f\"Parallel (4 Workers) Time: {par_time:.4f}s\")\n",
    "\n",
    "print(f\"\\n>>> Speedup: {seq_time / par_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-3-examples",
   "metadata": {},
   "source": [
    "### **More Examples: Parallelizing Work**\n",
    "\n",
    "**Example 1: Mass Image Resizing (CPU Bound)**\n",
    "You have 10,000 high-res images to resize.\n",
    "* **Sequential:** `for img in *.jpg; do convert $img ...; done`. (Uses 1 Core. Takes hours.)\n",
    "* **Parallel:** `ls *.jpg | xargs -P 8 -n 1 convert ...` (Uses 8 Cores. Takes minutes.)\n",
    "\n",
    "**Example 2: Checking Website Status (I/O Bound)**\n",
    "You have a list of 1,000 URLs to ping.\n",
    "* **Sequential:** Ping one, wait for reply, ping next. Most time is spent waiting on the network.\n",
    "* **Parallel:** `cat urls.txt | xargs -P 50 -n 1 curl -I`. Launch 50 pings at once. The CPU is mostly idle anyway, so you can run many threads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-4-header",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-4-intro",
   "metadata": {},
   "source": [
    "## **üõ†Ô∏è Level 4: Process Substitution (Named Pipes)**\n",
    "\n",
    "**Theory Recap:** Sometimes a tool *needs* a file argument (like `diff file1 file2`), but your data is in a stream. You don't want to create temp files.\n",
    "\n",
    "**The Trick:** `diff <(cmd1) <(cmd2)`.\n",
    "The shell creates a temporary **Named Pipe** (e.g., `/dev/fd/63`) that acts like a file but is actually a RAM buffer.\n",
    "\n",
    "**Note:** This is a Shell feature. We will demonstrate it using `%%bash`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "code-4-proc-sub",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Process Substitution Demo ---\n",
      "1c1\n",
      "< Hello\n",
      "---\n",
      "> World\n",
      "\\n‚úÖ Diff executed without intermediate files.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Only runs on Linux/Mac/WSL. Windows CMD/Powershell does not support this syntax directly.\n",
    "if [ \"$(uname)\" == \"Darwin\" ] || [ \"$(uname)\" == \"Linux\" ]; then\n",
    "    echo \"--- Process Substitution Demo ---\"\n",
    "    \n",
    "    # We want to compare the output of two commands without saving files.\n",
    "    # Command 1: echo \"Hello\"\n",
    "    # Command 2: echo \"World\"\n",
    "    \n",
    "    # diff expects files. We give it pipes.\n",
    "    diff <(echo \"Hello\") <(echo \"World\")\n",
    "    \n",
    "    echo \"\\n‚úÖ Diff executed without intermediate files.\"\n",
    "else\n",
    "    echo \"Skipping Process Substitution (Not supported on Windows Native)\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-4-examples",
   "metadata": {},
   "source": [
    "### **More Examples: Process Substitution**\n",
    "\n",
    "**Example 1: Verifying Remote Files**\n",
    "Compare a local config file with a remote one without downloading.\n",
    "`diff config.xml <(ssh user@server 'cat /etc/config.xml')`\n",
    "* The local `diff` tool reads your local file AND the stream coming from SSH as if it were a second file.\n",
    "\n",
    "**Example 2: Merging Sorted Streams**\n",
    "You have two commands producing sorted data. You want to merge them.\n",
    "`sort -m <(generate_data_a | sort) <(generate_data_b | sort)`\n",
    "* `sort -m` (merge) expects files. We feed it two live streams. It merges them on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-footer",
   "metadata": {},
   "source": [
    "## **üßπ Cleanup**\n",
    "Removing temporary files created during the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "code-cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for f in [INPUT_FILE, TEMP_FILE, TOOL_FILE]:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "print(\"Cleanup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bccf1b3-1a1c-4cfa-8443-d5f672962435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
